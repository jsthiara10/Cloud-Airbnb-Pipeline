# ğŸ¡ Airbnb NYC Data Pipeline â€“ GCP Production Rollout #

This project delivers a scalable, modular data pipeline to clean and load NYC Airbnb listings into Google BigQuery. It supports both local testing and production-grade GCP deployment.

### ğŸ“ Project Structure
```
.
â”œâ”€â”€ pipeline.py               # Main data cleaning logic
â”œâ”€â”€ main.py                   # GCP Cloud Function entrypoint
â”œâ”€â”€ utils.py                  # Reusable helpers (validation, quoting, etc.)
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ /data/
â”‚   â”œâ”€â”€ raw/                  # Local raw CSV files
â”‚   â””â”€â”€ cleaned/              # Locally cleaned output files
â”œâ”€â”€ /logs/                    # Local run logs
â”œâ”€â”€ /gcf/                     # GCP deployment config
â””â”€â”€ README.md                 # This file
```

## âœ… Key Features
ğŸ”„ End-to-end CSV ingestion, cleaning, and BigQuery loading

ğŸ” Cleans name and host_name fields and safely encloses them in double quotes

ğŸ“¦ Prevents column shifting by quoting all values

## ğŸ§¼ Removes:

Duplicate rows

Null values

Listings with 0 reviews

ğŸ§  Validates DataFrame to remove rogue index-like columns

ğŸ§ª Fully testable on-premises, deployable to GCP

## ğŸ› ï¸ Infrastructure:
### This project assumes you have:

A raw GCS bucket (for unprocessed files)

A clean GCS bucket (for cleaned files)

A BigQuery dataset and table for loading cleaned data

âœ… Optional: Buckets and other resources can be provisioned using Terraform (terraform/) or manually created in the GCP Console.

### âš™ï¸ Local Usage
### Step 1: Run the cleaning pipeline locally
```
python pipeline.py \
  --input data/raw/AB_NYC_2019.csv \
  --output data/cleaned/AB_NYC_2019_cleaned.csv
```

### This will:

Remove duplicates and nulls

Enclose key fields in double quotes

Quote all fields

Save to data/cleaned/

Step 2: Upload to GCS (Raw Bucket)
```
gsutil cp data/cleaned/AB_NYC_2019_cleaned.csv gs://your-raw-bucket-name/
```

### If deployed, this triggers the Cloud Function automatically.

## â˜ï¸ GCP Cloud Function
When triggered, main.py:

Downloads the CSV from the raw bucket

Cleans the data via AirbnbCleaner

Uploads the result to the clean bucket

Loads the cleaned file into BigQuery

## ğŸ”„ BigQuery LoadJobConfig (Used in main.py)
```
job_config = bigquery.LoadJobConfig(
    source_format=bigquery.SourceFormat.CSV,
    skip_leading_rows=1,
    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
    autodetect=True,
    quote_character='"',
    max_bad_records=1000
)
```

## ğŸ” IAM Permissions
Ensure your Cloud Function service account has:

Role & Purpose:
roles/storage.objectViewer Read from raw bucket
roles/storage.objectCreator	Write to clean bucket
roles/bigquery.dataEditor	Load data into BigQuery

## ğŸ› ï¸ Manual Import into BigQuery
Use the bq CLI if you want to load the cleaned CSV manually:

```
bq load \
  --source_format=CSV \
  --skip_leading_rows=1 \
  --replace \
  --quote='"' \
  --max_bad_records=1000 \
  project_id:dataset.table \
  ./data/cleaned/AB_NYC_2019_cleaned.csv
```

## ğŸ“Œ Common Issues Solved
Problem	Solution
Shifting columns / wrong data alignment	Use quotechar='"' and quoting=csv.QUOTE_ALL
Broken rows / too many values	Enclose name and host_name in quotes
Rogue index columns	Utility added to drop index-like or unnamed columns
Encoding problems	Ensured consistent UTF-8 encoding

## ğŸ§° Future Improvements
 Add unit tests (pytest)

 Add DAG scheduling with Cloud Composer or Scheduler

 Add monitoring & alerting (Cloud Monitoring)

 Support for JSON and semi-structured formats

## ğŸ™Œ Acknowledgments
Thanks to everyone who contributed ideas, tests, and bugs during the pipelineâ€™s PoC phase! Suggestions for improvement are welcome.

